"""
agent.py - Enhanced version with graph explanation and research conclusion capabilities
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import os
import json
import aiohttp
import pandas as pd
from datetime import datetime
import traceback
import tiktoken
from collections import Counter

from dotenv import load_dotenv
from database import Neo4jDatabase
import litellm

# --- Boilerplate and Helpers ---
load_dotenv()
DEBUG_MODE = False
TOKEN_ENCODER = None


def get_token_encoder():
    global TOKEN_ENCODER
    if TOKEN_ENCODER is None:
        TOKEN_ENCODER = tiktoken.get_encoding("cl100k_base")
    return TOKEN_ENCODER


def count_tokens(text: str) -> int:
    if not text:
        return 0
    return len(get_token_encoder().encode(text))


def set_debug_mode(enabled: bool):
    global DEBUG_MODE
    DEBUG_MODE = enabled


def get_debug_mode() -> bool:
    return DEBUG_MODE


def debug_log(message: str, data: Any = None):
    if DEBUG_MODE:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[DEBUG {timestamp}] {message}")
        if data:
            print(f"[DEBUG DATA] {json.dumps(data, indent=2, default=str)}")


def add_debug_output(message: str):
    debug_log(message)



class OllamaModelManager:
    def __init__(
        self, ollama_url: str = os.getenv("OLLAMA_API_BASE", "http://ollama:11434")
    ):
        self.ollama_url = ollama_url

    async def get_available_models(self) -> List[str]:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_url}/api/tags") as resp:
                    if resp.status == 200:
                        return [m["name"] for m in (await resp.json()).get("models", [])]
            return []
        except Exception as e:
            debug_log(f"Error getting models: {e}")
            return []
 
    async def check_ollama_status(self) -> bool:
        try:
            async with aiohttp.ClientSession() as s:
                async with s.get(f"{self.ollama_url}/api/tags", timeout=5) as r:
                    return r.status == 200
        except Exception:
            return False

    async def pull_model(self, model_name: str) -> Dict[str, Any]:
        try:
            debug_log(f"Pulling model: {model_name}")
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_url}/api/pull", json={"name": model_name}
                ) as response:

                    if response.status != 200:
                        error_text = await response.text()
                        debug_log(f"Model pull failed: {error_text}")
                        return {
                            "success": False,
                            "message": f"Failed to pull model: {error_text}",
                        }

                    progress_updates = []
                    async for line in response.content:
                        if line.strip():
                            try:
                                update = json.loads(line.decode())
                                progress_updates.append(update)
                                debug_log(f"Model pull progress: {update}")
                            except json.JSONDecodeError:
                                continue

                    return {
                        "success": True,
                        "message": f"Model '{model_name}' pulled successfully",
                    }

        except Exception as e:
            debug_log(f"Error during model pull: {str(e)}")
            return {"success": False, "message": f"Error: {str(e)}"}

    def get_recommended_models(self) -> List[str]:
        return ["gemma3:4b", "gemma3:1b", "deepseek-r1:1.5b", "qwen3:0.6b"]


# --- Enhanced Core Agent Logic ---
class AnalysisAgent:
    def __init__(self, db: Optional[Neo4jDatabase] = None):
        self.db = db if db is not None else Neo4jDatabase()
        self.model = "ollama/gemma3:1b"
        self.model_manager = OllamaModelManager()
        self.max_prompt_tokens = 3800
        
        # Enhanced system prompts for different analysis types
        self.security_prompt = (
            "You are a cybersecurity expert. Analyze the provided JSON summary of Active Directory authentication data. "
            "Focus on interpreting the `stats` and `anomaly_summary` to identify threats like lateral movement, red team activity, and high-risk users. "
            "Your response must be a valid JSON object containing the following keys: 'findings' (list of strings), "
            "'suspicious_activities' (list of strings), 'recommendations' (list of strings), "
            "'summary' (a concise executive summary string), and 'risk_level' (string: LOW, MEDIUM, HIGH, CRITICAL). "
            "Ensure 'summary' is always present and a non-empty string."
        )
        
        self.graph_analysis_prompt = (
            "You are a data analyst specializing in graph analysis and network security visualization. "
            "Analyze the provided graph data including network topology, risk patterns, and temporal data. "
            "Explain what each type of visualization reveals about the security posture. "
            "Your response must be a valid JSON object with: 'network_insights' (list), 'risk_patterns' (list), "
            "'temporal_findings' (list), 'graph_summary' (string), and 'key_metrics' (dict)."
        )
        
        self.research_prompt = (
            "You are a cybersecurity researcher writing conclusions for an academic paper on the security breach security analysis. "
            "Based on the provided analysis data, generate research conclusions including methodology, findings, and implications. "
            "Your response must be a valid JSON object with: 'methodology_summary' (string), 'key_findings' (list), "
            "'statistical_insights' (list), 'limitations' (list), 'future_research' (list), and 'conclusion' (string)."
        )

    def set_model(self, model_name: str):
        if not model_name.startswith("ollama/"):
            model_name = f"ollama/{model_name}"
        self.model = model_name

    def get_current_model(self) -> str:
        return self.model.replace("ollama/", "")

    def execute_query(
        self, query: str, parameters: Dict[str, Any] = None, database: str = "authdata"
    ):
        try:
            result = self.db.execute_query(query, parameters, database)
            debug_log(
                f"Query returned {len(result.get('records', []))} records",
                {"query": query.split("\n", 1)[0]},
            )
            return result
        except Exception as e:
            debug_log(f"Neo4j query error: {str(e)}", {"query": query})
            return {"success": False, "records": [], "error": str(e)}


    async def analyze_with_graph(self, scan_type: str = "quick") -> Dict[str, Any]:
        try:
            limit = 2000 if scan_type == "full" else 500

            events_query = f"MATCH (u:User)<-[:FROM_USER]-(ae:AuthEvent)-[:TO_COMPUTER]->(c:Computer) RETURN u.name as username, c.name as computer_name, ae.success as success, ae.logon_type as logon_type, ae.auth_type as auth_type, ae.is_redteam as is_redteam, ae.timestamp as timestamp ORDER BY ae.timestamp DESC LIMIT {limit}"
            events_result = self.execute_query(events_query)

            if not events_result.get("records"):
                return {
                    "summary": "Analysis failed: No valid, connected authentication data was found.",
                    "risk_level": "UNKNOWN",
                    "findings": ["No usable authentication events in the database."],
                    "recommendations": [
                        "Ensure your data import script has run successfully and created relationships."
                    ],
                }

            records = events_result["records"]
            anomaly_summary = self._create_anomaly_summary(records)

            prompt = self._build_llm_prompt(records, anomaly_summary)
            token_count = count_tokens(prompt)
            debug_log(f"Final prompt size: {token_count} tokens")
            
            if token_count > self.max_prompt_tokens:
                raise ValueError(
                    f"Prompt is too large ({token_count} tokens). The anomaly summary is too verbose."
                )

            response = await litellm.acompletion(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.security_prompt},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.2,
                max_tokens=2000,
                format="json",
            )

            content = response.choices[0].message.content
            try:
                if content.strip().startswith("```json"):
                    content = content.strip()[7:-4]
                result = json.loads(content)
            except json.JSONDecodeError as e:
                raise ValueError(f"AI model returned malformed JSON. Error: {e}")

            result["anomaly_scores"] = anomaly_summary.get("anomaly_scores", {})
            return result

        except Exception as e:
            error_msg = f"Analysis error: {str(e)}"
            debug_log(error_msg, {"traceback": traceback.format_exc()})
            return {
                "summary": f"A critical error occurred: {str(e)}",
                "risk_level": "UNKNOWN",
                "findings": [error_msg],
            }

    async def analyze_graphs(self) -> Dict[str, Any]:
        """Analyze visualization data and provide insights about each graph type"""
        try:
            # Get data for all visualization types
            # Use 'self' (the AnalysisAgent instance) when calling helper functions
            network_data = await get_graph_for_visualization(self)
            risk_data = await get_user_behavior_data(self)
            temporal_data = await get_hourly_data(self)
            
            # Create analysis summary
            analysis_data = {
                "network_topology": {
                    "total_connections": len(network_data.get("data", [])),
                    "unique_entities": len(set([d.get("user_name") for d in network_data.get("data", [])] + 
                                            [d.get("computer_name") for d in network_data.get("data", [])]))
                },
                "risk_patterns": {
                    "users_analyzed": len(risk_data.get("data", [])),
                    "high_risk_users": len([u for u in risk_data.get("data", []) 
                                          if u.get("fails", 0) / max(u.get("total", 1), 1) > 0.1])
                },
                "temporal_patterns": {
                    "hours_analyzed": len(temporal_data.get("data", {}).get("hourly_data", [])),
                    "peak_hour": max(temporal_data.get("data", {}).get("hourly_data", []), 
                                   key=lambda x: x.get("event_count", 0), default={}).get("hour", "N/A")
                }
            }
            
            prompt = f"Analyze these graph visualization insights: {json.dumps(analysis_data, separators=(',', ':'))}"
            
            response = await litellm.acompletion(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.graph_analysis_prompt},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
                max_tokens=1500,
                format="json",
            )

            content = response.choices[0].message.content
            if content.strip().startswith("```json"):
                content = content.strip()[7:-4]
            
            return json.loads(content)

        except Exception as e:
            debug_log(f"Graph analysis error: {str(e)}")
            return {
                "network_insights": [f"Error analyzing network: {str(e)}"],
                "risk_patterns": ["Unable to analyze risk patterns"],
                "temporal_findings": ["Unable to analyze temporal data"],
                "graph_summary": f"Graph analysis failed: {str(e)}",
                "key_metrics": {}
            }

    async def generate_research_conclusions(self, security_analysis: Dict, graph_analysis: Dict) -> Dict[str, Any]:
        """Generate research paper conclusions based on security and graph analysis"""
        try:
            research_data = {
                "security_findings": {
                    "risk_level": security_analysis.get("risk_level", "UNKNOWN"),
                    "findings_count": len(security_analysis.get("findings", [])),
                    "suspicious_activities": len(security_analysis.get("suspicious_activities", [])),
                    "anomaly_users": len(security_analysis.get("anomaly_scores", {}))
                },
                "graph_insights": {
                    "network_patterns": len(graph_analysis.get("network_insights", [])),
                    "risk_indicators": len(graph_analysis.get("risk_patterns", [])),
                    "temporal_anomalies": len(graph_analysis.get("temporal_findings", []))
                },
                "methodology": "Multi-dimensional security breach analysis using graph databases and ML"
            }
            
            prompt = f"Generate research conclusions based on this analysis: {json.dumps(research_data, separators=(',', ':'))}"
            
            response = await litellm.acompletion(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.research_prompt},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.4,
                max_tokens=2000,
                format="json",
            )

            content = response.choices[0].message.content
            if content.strip().startswith("```json"):
                content = content.strip()[7:-4]
            
            return json.loads(content)

        except Exception as e:
            debug_log(f"Research conclusion error: {str(e)}")
            return {
                "methodology_summary": f"Analysis methodology encountered error: {str(e)}",
                "key_findings": ["Unable to generate findings due to processing error"],
                "statistical_insights": ["Statistical analysis unavailable"],
                "limitations": [f"Analysis limited by error: {str(e)}"],
                "future_research": ["Error recovery and system optimization needed"],
                "conclusion": f"Research analysis incomplete due to technical issues: {str(e)}"
            }

    def _create_anomaly_summary(self, records: List[Dict]) -> Dict:
        """Create compact statistical summary for analysis"""
        redteam_events = [r for r in records if r.get("is_redteam")]

        # Handle timestamp objects safely
        off_hours_events = []
        for r in records:
            timestamp = r.get("timestamp")
            if timestamp:
                try:
                    if hasattr(timestamp, "hour"):
                        hour = timestamp.hour
                    elif hasattr(timestamp, "to_native"):
                        native_dt = timestamp.to_native()
                        hour = native_dt.hour
                    elif isinstance(timestamp, str):
                        dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
                        hour = dt.hour
                    else:
                        continue

                    if hour >= 22 or hour <= 6:
                        off_hours_events.append(r)
                except (AttributeError, ValueError, TypeError):
                    continue

        user_failure_counts = Counter()
        user_computer_counts = Counter()
        for r in records:
            if r.get("success") == "Failure" and r.get("username"):
                user_failure_counts[r["username"]] += 1
            if r.get("username"):
                user_computer_counts[r["username"]] += 1

        high_failure_users = {
            user: count for user, count in user_failure_counts.items() if count > 10
        }
        multi_computer_users = {
            user: count for user, count in user_computer_counts.items() if count > 5
        }

        return {
            "redteam_summary": {
                "count": len(redteam_events),
                "top_users": [
                    u for u, c in Counter(
                        r.get("username") for r in redteam_events
                    ).most_common(5)
                ],
                "top_computers": [
                    c for c, c_ in Counter(
                        r.get("computer_name") for r in redteam_events
                    ).most_common(5)
                ],
            },
            "off_hours_summary": {
                "count": len(off_hours_events),
                "top_users": [
                    u for u, c in Counter(
                        r.get("username") for r in off_hours_events
                    ).most_common(5)
                ],
            },
            "high_failure_summary": {
                "count": len(high_failure_users),
                "top_offenders": dict(Counter(high_failure_users).most_common(5)),
            },
            "multiple_computer_access_summary": {
                "count": len(multi_computer_users),
                "top_users": dict(Counter(multi_computer_users).most_common(5)),
            },
            "anomaly_scores": {
                user: count for user, count in user_failure_counts.most_common(20)
            },
        }

    def _build_llm_prompt(self, records: List[Dict], anomaly_summary: Dict) -> str:
        stats = {
            "total_events_scanned": len(records),
            "failed_logins_scanned": sum(
                1 for r in records if r.get("success") == "Failure"
            ),
            "unique_users_scanned": len(
                set(r.get("username") for r in records if r.get("username"))
            ),
        }

        return (
            f"Analyze this security breach data. Focus on interpreting the `stats` and `anomaly_summary`.\n"
            f"**Stats:**\n{json.dumps(stats, separators=(',', ':'))}\n"
            f"**Anomaly Summary:**\n{json.dumps(anomaly_summary, default=str, separators=(',', ':'))}\n"
            f"Based on these summaries, identify findings, suspicious activities, and recommendations."
        )

    def close(self):
        if self.connection:
            self.connection.close()


# --- Interface Functions ---
async def check_llm_status(agent: AnalysisAgent) -> bool:
    try:
        await litellm.acompletion(
            model=agent.model,
            messages=[{"role": "user", "content": "ping"}],
            max_tokens=5,
            timeout=10,
        )
        return True
    except Exception:
        return False


async def get_available_models(agent: AnalysisAgent) -> List[str]:
    return await agent.model_manager.get_available_models()


def get_recommended_models(agent: AnalysisAgent) -> List[str]:
    return agent.model_manager.get_recommended_models()


def set_current_model(agent: AnalysisAgent, model_name: str):
    agent.set_model(model_name)


def get_current_model(agent: AnalysisAgent) -> str:
    return agent.get_current_model()


async def run_full_scan(agent: AnalysisAgent) -> str:
    return format_analysis_result(await agent.analyze_with_graph("full"))


async def run_quick_scan(agent: AnalysisAgent) -> str:
    return format_analysis_result(await agent.analyze_with_graph("quick"))


# --- New Enhanced Analysis Functions ---
async def analyze_graphs_with_agent(agent: AnalysisAgent) -> str:
    """Get agent's analysis of the visualization graphs"""
    result = await agent.analyze_graphs()
    # Graph analysis has its own formatter for display
    try:
        return format_graph_analysis(result)
    except Exception as e:
        debug_log(f"Error formatting graph analysis: {e}")
        return f"❌ Error formatting graph analysis: {e}"


async def generate_research_conclusions_with_agent(agent: AnalysisAgent) -> str:
    """Get agent's research conclusions based on previous analyses"""
    security_analysis = await agent.analyze_with_graph("full")
    graph_analysis = await agent.analyze_graphs()
    research_result = await agent.generate_research_conclusions(security_analysis, graph_analysis)
    
    # Research conclusions have their own formatter
    try:
        return format_research_conclusions(research_result)
    except Exception as e:
        debug_log(f"Error formatting research conclusions: {e}")
        return f"❌ Error formatting research conclusions: {e}"


def format_analysis_result(result: Dict[str, Any]) -> str:
    try:
        risk = result.get("risk_level", "UNKNOWN").upper()
        emoji = {"LOW": "🟢", "MEDIUM": "🟡", "HIGH": "🟠", "CRITICAL": "🔴"}.get(risk, "⚪")
        output = [
            f"## {emoji} Risk Level: {risk}\n",
            f"### 📋 Executive Summary\n{result.get('summary', 'N/A')}\n",
        ]
        if result.get("findings"):
            output.extend(
                ["### 🔍 Key Findings", *[f"- {f}" for f in result["findings"]], ""]
            )
        if result.get("suspicious_activities"):
            output.extend(
                [
                    "### ⚠️ Suspicious Activities",
                    *[f"- {a}" for a in result["suspicious_activities"]],
                    "",
                ]
            )
        if result.get("anomaly_scores"):
            scores = sorted(
                result["anomaly_scores"].items(), key=lambda item: item[1], reverse=True
            )
            output.extend(
                [
                    "### 📊 Anomaly Scores (Top Users by Failure Count)",
                    *[f"- **{entity}**: {score} failures" for entity, score in scores[:10]],
                    "",
                ]
            )
        if result.get("recommendations"):
            output.extend(
                ["### 💡 Recommendations", *[f"- {r}" for r in result["recommendations"]]]
            )
        return "\n".join(output)
    except Exception as e:
        debug_log(f"Error formatting results: {str(e)}")
        return f"❌ Error formatting: {str(e)}\n\nRaw: {json.dumps(result, indent=2, default=str)}"


def format_graph_analysis(result: Dict[str, Any]) -> str:
    """Format graph analysis results for display"""
    try:
        output = [
            "# 📊 Graph Analysis Report\n",
            f"## 🔗 Network Topology Insights\n{result.get('graph_summary', 'N/A')}\n"
        ]
        
        if result.get("network_insights"):
            output.extend([
                "### Network Patterns:",
                *[f"- {insight}" for insight in result["network_insights"]],
                ""
            ])
        
        if result.get("risk_patterns"):
            output.extend([
                "### Risk Heatmap Analysis:",
                *[f"- {pattern}" for pattern in result["risk_patterns"]],
                ""
            ])
        
        if result.get("temporal_findings"):
            output.extend([
                "### Temporal Pattern Analysis:",
                *[f"- {finding}" for finding in result["temporal_findings"]],
                ""
            ])
        
        if result.get("key_metrics"):
            output.extend([
                "### Key Metrics:",
                *[f"- **{k}**: {v}" for k, v in result["key_metrics"].items()],
                ""
            ])
        
        return "\n".join(output)
    except Exception as e:
        return f"❌ Error formatting graph analysis: {str(e)}"


def format_research_conclusions(result: Dict[str, Any]) -> str:
    """Format research conclusions for display"""
    try:
        output = [
            "# 🎓 Research Paper Conclusions\n",
            f"## Methodology\n{result.get('methodology_summary', 'N/A')}\n"
        ]
        
        if result.get("key_findings"):
            output.extend([
                "## Key Findings:",
                *[f"- {finding}" for finding in result["key_findings"]],
                ""
            ])
        
        if result.get("statistical_insights"):
            output.extend([
                "## Statistical Insights:",
                *[f"- {insight}" for insight in result["statistical_insights"]],
                ""
            ])
        
        if result.get("limitations"):
            output.extend([
                "## Study Limitations:",
                *[f"- {limitation}" for limitation in result["limitations"]],
                ""
            ])
        
        if result.get("future_research"):
            output.extend([
                "## Future Research Directions:",
                *[f"- {direction}" for direction in result["future_research"]],
                ""
            ])
        
        output.extend([
            f"## Conclusion\n{result.get('conclusion', 'N/A')}"
        ])
        
        return "\n".join(output)
    except Exception as e:
        return f"❌ Error formatting research conclusions: {str(e)}"


# --- Visualization Queries (Schema-Aligned) ---
async def get_graph_for_visualization(agent: AnalysisAgent):
    query = "MATCH (u:User)<-[:FROM_USER]-(evt:AuthEvent)-[:TO_COMPUTER]->(c:Computer) RETURN u.name AS user_name, c.name AS computer_name, count(evt) AS connection_events ORDER BY connection_events DESC LIMIT 200"
    result = agent.execute_query(query)
    return {"success": result["success"], "data": result.get("records", [])}


async def get_hourly_data(agent: AnalysisAgent):
    query = "MATCH (a:AuthEvent) WHERE a.timestamp IS NOT NULL RETURN a.timestamp.hour as hour, count(*) as event_count ORDER BY hour"
    result = agent.execute_query(query)
    return {
        "success": result["success"],
        "data": {"hourly_data": result.get("records", [])},
    }


async def get_user_behavior_data(agent: AnalysisAgent):
    query = "MATCH (u:User)<-[:FROM_USER]-(ae:AuthEvent) RETURN u.name as username, count(ae) as total, sum(CASE WHEN ae.success = 'Failure' THEN 1 ELSE 0 END) as fails ORDER BY total DESC LIMIT 100"
    result = agent.execute_query(query)
    return {"success": result["success"], "data": result.get("records", [])}